{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Імпорт бібліотек\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 1. Завантаження та попередня обробка даних\n",
    "data = pd.read_csv('creditcard.csv')\n",
    "print(data.head())\n",
    "\n",
    "# Масштабування ознак\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data.drop('Class', axis=1))\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_scaled, data['Class'], test_size=0.2, random_state=42)\n",
    "\n",
    "# 2. Метод Isolation Forest\n",
    "model_if = IsolationForest(contamination=0.01, random_state=42)\n",
    "y_pred_if = model_if.fit_predict(X_test)\n",
    "y_pred_if = np.where(y_pred_if == -1, 1, 0)  # -1 означає аномалію\n",
    "print(\"Метод Isolation Forest:\")\n",
    "print(classification_report(y_test, y_pred_if))\n",
    "\n",
    "# 3. Метод One-Class SVM\n",
    "model_svm = OneClassSVM(kernel='rbf', gamma=0.1, nu=0.01)\n",
    "model_svm.fit(X_train)\n",
    "y_pred_svm = model_svm.predict(X_test)\n",
    "y_pred_svm = np.where(y_pred_svm == -1, 1, 0)\n",
    "print(\"\\nМетод One-Class SVM:\")\n",
    "print(classification_report(y_test, y_pred_svm))\n",
    "\n",
    "# 4. Метод Local Outlier Factor \n",
    "model_lof = LocalOutlierFactor(n_neighbors=20, contamination=0.01)\n",
    "y_pred_lof = model_lof.fit_predict(X_test)\n",
    "y_pred_lof = np.where(y_pred_lof == -1, 1, 0)\n",
    "print(\"\\nМетод Local Outlier Factor:\")\n",
    "print(classification_report(y_test, y_pred_lof))\n",
    "\n",
    "# 5. Автоенкодер на основі PyTorch\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Параметри автоенкодера\n",
    "input_dim = X_train.shape[1]\n",
    "autoencoder = Autoencoder(input_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "num_epochs = 10\n",
    "\n",
    "# Навчання автоенкодера\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = autoencoder(torch.Tensor(X_train))\n",
    "    loss = criterion(outputs, torch.Tensor(X_train))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Виявлення аномалій за допомогою автоенкодера\n",
    "outputs = autoencoder(torch.Tensor(X_test))\n",
    "losses = torch.mean((outputs - torch.Tensor(X_test)) ** 2, dim=1).detach().numpy()\n",
    "threshold = np.percentile(losses, 99)  # Задаємо поріг як 99-й перцентиль\n",
    "y_pred_autoencoder = np.where(losses > threshold, 1, 0)\n",
    "\n",
    "\n",
    "print(\"\\nАвтоенкодер:\")\n",
    "print(classification_report(y_test, y_pred_autoencoder))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Звіт про результати тестування методів виявлення аномалій\n",
    "1. Найкращі результати\n",
    "На основі метрики classification_report для кожного методу (наприклад, Precision, Recall, F1-score) ми можемо порівняти їхню продуктивність. Часто метод, який має вищий F1-score для класу аномалій (Class = 1), вважається найбільш ефективним для виявлення шахрайства, оскільки цей клас менш розповсюджений і важливий для точної класифікації.\n",
    "\n",
    "2. Порівняння методів\n",
    "    Isolation Forest:\n",
    "\n",
    "Isolation Forest, який базується на випадкових деревах, часто демонструє хороші результати при виявленні аномалій у високовимірних даних. Цей метод інтуїтивно спрямований на виділення «аномальних» точок у просторах, де щільність даних значно нижча.\n",
    "Для даного набору даних Isolation Forest може показувати сильний результат, оскільки метод добре працює з великими вибірками, зберігаючи ефективність.\n",
    "    \n",
    "    One-Class SVM:\n",
    "\n",
    "One-Class SVM часто показує добрі результати в ситуаціях, коли дані важко розділити лінійно. У цьому випадку використання RBF kernel допомагає врахувати нелінійності в даних.\n",
    "Проте метод може бути чутливим до вибору параметра nu (який вказує на очікувану частку аномалій), і це може впливати на результати на нових даних.\n",
    "    \n",
    "    Local Outlier Factor (LOF):\n",
    "\n",
    "LOF показує хороші результати для виявлення локальних аномалій, оскільки враховує щільність сусідніх точок. Цей метод може бути ефективним для даних з локальними аномаліями, але може погано працювати з розрідженими даними.\n",
    "У нашому випадку LOF може бути менш ефективним, якщо шахрайські транзакції мають тенденцію до створення нових, але загалом розріджених кластерів.\n",
    "    \n",
    "    Автоенкодер:\n",
    "\n",
    "Глибокий нейронний автоенкодер вивчає стисле представлення даних, намагаючись відтворити вихідний вхід. Він може бути ефективним для аномалій, оскільки вивчає структуру даних і визначає елементи, які відхиляються від загальної схеми.\n",
    "Для великого обсягу даних автоенкодери можуть потребувати більше часу для навчання. Однак, якщо структура аномалій значно відрізняється від основної частини даних, автоенкодер може точно ідентифікувати аномальні точки за рахунок великих відмінностей у відтворювальній помилці.\n",
    "3. Аналіз продуктивності автоенкодера\n",
    "Автоенкодер як глибока нейронна мережа має потенціал для ефективного виявлення аномалій, коли набір даних складний і містить нелінійні зв’язки. Для нашої задачі автоенкодер дозволяє виявити аномалії, які мають значно більші відмінності від загальної структури даних. Його використання може бути виправданим у випадках, коли ми маємо складні або важкодоступні особливості даних, які інші методи (на кшталт Isolation Forest чи LOF) не можуть повністю врахувати. Проте автоенкодери потребують більших обчислювальних ресурсів та часу на тренування, що може бути обмеженням для їх використання в реальному часі."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Пояснення метрик\n",
    "Precision показує, який відсоток транзакцій, позначених як аномальні, дійсно є шахрайськими.\n",
    "\n",
    "Recall демонструє, який відсоток реальних шахрайських транзакцій було правильно позначено як аномальні.\n",
    "\n",
    "F1-score – це зважене середнє між Precision та Recall, яке є особливо корисним, коли важливий баланс між точністю та повнотою (Recall)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
